import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import os
import json
from datetime import datetime
import torch

# Importar m√≥dulos existentes
from mus_env import mus
from marl_agent import MARLAgent

class QuickEvaluationTool:
    def __init__(self, results_dir="training_results"):
        self.results_dir = results_dir
        self.evaluation_results = {}
        
        # Configurar estilo de gr√°ficos
        plt.style.use('seaborn-v0_8')
        sns.set_palette("husl")
    
    def load_trained_agents(self, model_path="best_models"):
        """Cargar agentes entrenados"""
        model_dir = os.path.join(self.results_dir, model_path)
        
        if not os.path.exists(model_dir):
            print(f"‚ùå No se encontraron modelos en {model_dir}")
            return None
        
        # Crear entorno para obtener dimensiones
        env = mus.env()
        
        # Crear agentes
        agents = {}
        for agent_id in env.possible_agents:
            agent = MARLAgent(
                state_size=env.observation_space(agent_id).shape[0],
                action_size=env.action_space(agent_id).n,
                agent_id=agent_id
            )
            
            # Cargar modelo entrenado
            model_file = os.path.join(model_dir, f"{agent_id}_best.pth")
            if os.path.exists(model_file):
                agent.q_network.load_state_dict(torch.load(model_file))
                agent.epsilon = 0.0  # Sin exploraci√≥n para evaluaci√≥n
                agents[agent_id] = agent
                print(f"‚úÖ Modelo cargado para {agent_id}")
            else:
                print(f"‚ö†Ô∏è No se encontr√≥ modelo para {agent_id}")
        
        return agents if agents else None
    
    def create_random_agents(self):
        """Crear agentes que act√∫an aleatoriamente para comparaci√≥n"""
        env = mus.env()
        
        class RandomAgent:
            def __init__(self, action_size):
                self.action_size = action_size
            
            def act(self, observation):
                return np.random.randint(0, self.action_size)
        
        random_agents = {}
        for agent_id in env.possible_agents:
            random_agents[agent_id] = RandomAgent(env.action_space(agent_id).n)
        
        return random_agents
    
    def evaluate_agents_vs_random(self, trained_agents, num_games=100):
        """Evaluar agentes entrenados contra agentes aleatorios"""
        print(f"üéØ Evaluando agentes entrenados vs aleatorios ({num_games} juegos)...")
        
        env = mus.env()
        random_agents = self.create_random_agents()
        
        results = {
            'trained_wins': 0,
            'random_wins': 0,
            'draws': 0,
            'game_lengths': [],
            'trained_scores': [],
            'random_scores': [],
            'strategic_actions': 0,
            'total_actions': 0
        }
        
        for game in range(num_games):
            env.reset()
            game_length = 0
            strategic_actions = 0
            
            # Alternar entre agentes entrenados y aleatorios
            # Equipo 0: entrenados, Equipo 1: aleatorios
            current_agents = {
                'player_0': trained_agents['player_0'],
                'player_1': random_agents['player_1'],
                'player_2': trained_agents['player_2'],
                'player_3': random_agents['player_3']
            }
            
            for agent_id in env.agent_iter():
                if env.terminations[agent_id] or env.truncations[agent_id]:
                    continue
                
                observation, reward, termination, truncation, info = env.last()
                game_length += 1
                
                if termination or truncation:
                    action = None
                else:
                    action = current_agents[agent_id].act(observation)
                    
                    # Contar acciones estrat√©gicas de agentes entrenados
                    if agent_id in ['player_0', 'player_2']:
                        if self._is_strategic_action(action, info):
                            strategic_actions += 1
                
                env.step(action)
            
            # Analizar resultado
            final_info = info
            scores = final_info.get('scores', [0, 0])
            
            results['game_lengths'].append(game_length)
            results['trained_scores'].append(scores[0])  # Equipo 0 (entrenados)
            results['random_scores'].append(scores[1])   # Equipo 1 (aleatorios)
            results['strategic_actions'] += strategic_actions
            results['total_actions'] += game_length
            
            if scores[0] > scores[1]:
                results['trained_wins'] += 1
            elif scores[1] > scores[0]:
                results['random_wins'] += 1
            else:
                results['draws'] += 1
        
        # Calcular m√©tricas finales
        results['trained_win_rate'] = results['trained_wins'] / num_games
        results['random_win_rate'] = results['random_wins'] / num_games
        results['draw_rate'] = results['draws'] / num_games
        results['avg_game_length'] = np.mean(results['game_lengths'])
        results['strategic_rate'] = results['strategic_actions'] / max(results['total_actions'], 1)
        results['avg_trained_score'] = np.mean(results['trained_scores'])
        results['avg_random_score'] = np.mean(results['random_scores'])
        
        self.evaluation_results['vs_random'] = results
        return results
    
    def evaluate_trained_vs_trained(self, trained_agents, num_games=50):
        """Evaluar agentes entrenados entre s√≠"""
        print(f"ü§ñ Evaluando agentes entrenados entre s√≠ ({num_games} juegos)...")
        
        env = mus.env()
        
        results = {
            'team_0_wins': 0,
            'team_1_wins': 0,
            'draws': 0,
            'game_lengths': [],
            'team_scores': {'team_0': [], 'team_1': []},
            'coordination_events': 0,
            'strategic_decisions': 0,
            'total_actions': 0
        }
        
        for game in range(num_games):
            env.reset()
            game_length = 0
            coordination_events = 0
            strategic_decisions = 0
            
            for agent_id in env.agent_iter():
                if env.terminations[agent_id] or env.truncations[agent_id]:
                    continue
                
                observation, reward, termination, truncation, info = env.last()
                game_length += 1
                
                if termination or truncation:
                    action = None
                else:
                    action = trained_agents[agent_id].act(observation)
                    
                    if self._is_strategic_action(action, info):
                        strategic_decisions += 1
                    
                    if self._indicates_coordination(agent_id, action, info):
                        coordination_events += 1
                
                env.step(action)
            
            # Analizar resultado
            final_info = info
            scores = final_info.get('scores', [0, 0])
            
            results['game_lengths'].append(game_length)
            results['team_scores']['team_0'].append(scores[0])
            results['team_scores']['team_1'].append(scores[1])
            results['coordination_events'] += coordination_events
            results['strategic_decisions'] += strategic_decisions
            results['total_actions'] += game_length
            
            if scores[0] > scores[1]:
                results['team_0_wins'] += 1
            elif scores[1] > scores[0]:
                results['team_1_wins'] += 1
            else:
                results['draws'] += 1
        
        # Calcular m√©tricas finales
        results['team_0_win_rate'] = results['team_0_wins'] / num_games
        results['team_1_win_rate'] = results['team_1_wins'] / num_games
        results['draw_rate'] = results['draws'] / num_games
        results['avg_game_length'] = np.mean(results['game_lengths'])
        results['coordination_rate'] = results['coordination_events'] / max(results['total_actions'], 1)
        results['strategic_rate'] = results['strategic_decisions'] / max(results['total_actions'], 1)
        results['balance_score'] = abs(results['team_0_win_rate'] - 0.5)  # Qu√© tan lejos del equilibrio
        
        self.evaluation_results['trained_vs_trained'] = results
        return results
    
    def _is_strategic_action(self, action, info):
        """Determinar si una acci√≥n es estrat√©gica"""
        phase = info.get('phase', 'unknown')
        
        strategic_actions = {
            'mus': [2, 3],  # mus, no mus
            'grande': [1, 6, 7],  # envido, √≥rdago, quiero
            'chica': [1, 6, 7],
            'pares': [1, 6, 7],
            'juego': [1, 6, 7],
            'descarte': list(range(11, 15))  # seleccionar cartas
        }
        
        return action in strategic_actions.get(phase, [])
    
    def _indicates_coordination(self, agent_id, action, info):
        """Detectar indicios de coordinaci√≥n (simplificado)"""
        phase = info.get('phase', 'unknown')
        
        # Acciones que podr√≠an indicar coordinaci√≥n en fases de apuesta
        if phase in ['grande', 'chica', 'pares', 'juego']:
            return action in [1, 6, 7]  # envido, √≥rdago, quiero
        
        return False
    
    def generate_evaluation_report(self):
        """Generar informe visual de evaluaci√≥n"""
        if not self.evaluation_results:
            print("‚ùå No hay resultados de evaluaci√≥n para mostrar")
            return
        
        # Crear figura con m√∫ltiples subplots
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('üéØ Informe de Evaluaci√≥n de Agentes MARL', fontsize=16, fontweight='bold')
        
        # 1. Comparaci√≥n de tasas de victoria
        if 'vs_random' in self.evaluation_results:
            self._plot_win_rates_comparison(axes[0, 0])
        
        # 2. Distribuci√≥n de puntuaciones
        if 'vs_random' in self.evaluation_results:
            self._plot_score_distribution(axes[0, 1])
        
        # 3. Duraci√≥n de juegos
        self._plot_game_lengths(axes[0, 2])
        
        # 4. M√©tricas de calidad
        self._plot_quality_metrics(axes[1, 0])
        
        # 5. An√°lisis de coordinaci√≥n
        if 'trained_vs_trained' in self.evaluation_results:
            self._plot_coordination_analysis(axes[1, 1])
        
        # 6. Resumen de rendimiento
        self._plot_performance_summary(axes[1, 2])
        
        plt.tight_layout()
        
        # Guardar informe
        report_path = os.path.join(self.results_dir, f"evaluation_report_{datetime.now().strftime('%Y%m%d_%H%M')}.png")
        plt.savefig(report_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"üìä Informe de evaluaci√≥n guardado: {report_path}")
        
        # Generar resumen textual
        self._generate_text_summary()
    
    def _plot_win_rates_comparison(self, ax):
        """Gr√°fico de comparaci√≥n de tasas de victoria"""
        ax.set_title('üèÜ Tasas de Victoria: Entrenados vs Aleatorios', fontweight='bold')
        
        results = self.evaluation_results['vs_random']
        
        categories = ['Entrenados', 'Aleatorios', 'Empates']
        values = [results['trained_win_rate'], results['random_win_rate'], results['draw_rate']]
        colors = ['green', 'red', 'gray']
        
        bars = ax.bar(categories, values, color=colors, alpha=0.7)
        
        # A√±adir valores en las barras
        for bar, value in zip(bars, values):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                   f'{value:.2%}', ha='center', va='bottom', fontweight='bold')
        
        ax.set_ylabel('Tasa de Victoria')
        ax.set_ylim(0, 1)
        ax.grid(True, alpha=0.3)
    
    def _plot_score_distribution(self, ax):
        """Distribuci√≥n de puntuaciones"""
        ax.set_title('üìä Distribuci√≥n de Puntuaciones', fontweight='bold')
        
        results = self.evaluation_results['vs_random']
        
        ax.hist(results['trained_scores'], alpha=0.7, label='Entrenados', bins=15, color='green')
        ax.hist(results['random_scores'], alpha=0.7, label='Aleatorios', bins=15, color='red')
        
        ax.axvline(np.mean(results['trained_scores']), color='green', linestyle='--', 
                  label=f'Media Entrenados: {np.mean(results["trained_scores"]):.1f}')
        ax.axvline(np.mean(results['random_scores']), color='red', linestyle='--',
                  label=f'Media Aleatorios: {np.mean(results["random_scores"]):.1f}')
        
        ax.legend()
        ax.set_xlabel('Puntuaci√≥n')
        ax.set_ylabel('Frecuencia')
        ax.grid(True, alpha=0.3)
    
    def _plot_game_lengths(self, ax):
        """Duraci√≥n de los juegos"""
        ax.set_title('‚è±Ô∏è Duraci√≥n de los Juegos', fontweight='bold')
        
        all_lengths = []
        labels = []
        
        if 'vs_random' in self.evaluation_results:
            all_lengths.append(self.evaluation_results['vs_random']['game_lengths'])
            labels.append('vs Aleatorios')
        
        if 'trained_vs_trained' in self.evaluation_results:
            all_lengths.append(self.evaluation_results['trained_vs_trained']['game_lengths'])
            labels.append('Entrenados vs Entrenados')
        
        if all_lengths:
            ax.boxplot(all_lengths, labels=labels)
            ax.set_ylabel('Duraci√≥n (acciones)')
            ax.grid(True, alpha=0.3)
    
    def _plot_quality_metrics(self, ax):
        """M√©tricas de calidad de decisiones"""
        ax.set_title('üéØ Calidad de Decisiones', fontweight='bold')
        
        metrics = []
        values = []
        
        if 'vs_random' in self.evaluation_results:
            metrics.append('Decisiones\nEstrat√©gicas')
            values.append(self.evaluation_results['vs_random']['strategic_rate'])
        
        if 'trained_vs_trained' in self.evaluation_results:
            metrics.extend(['Coordinaci√≥n\nde Equipo', 'Balance\nCompetitivo'])
            values.extend([
                self.evaluation_results['trained_vs_trained']['coordination_rate'],
                1 - self.evaluation_results['trained_vs_trained']['balance_score']  # Invertir para que mayor sea mejor
            ])
        
        if metrics:
            bars = ax.bar(metrics, values, alpha=0.7, color=['blue', 'orange', 'purple'][:len(metrics)])
            
            for bar, value in zip(bars, values):
                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                       f'{value:.2%}', ha='center', va='bottom', fontweight='bold')
            
            ax.set_ylabel('Tasa')
            ax.set_ylim(0, 1)
            ax.grid(True, alpha=0.3)
    
    def _plot_coordination_analysis(self, ax):
        """An√°lisis de coordinaci√≥n entre equipos"""
        ax.set_title('ü§ù An√°lisis de Coordinaci√≥n', fontweight='bold')
        
        if 'trained_vs_trained' not in self.evaluation_results:
            ax.text(0.5, 0.5, 'No hay datos\nde coordinaci√≥n', ha='center', va='center', transform=ax.transAxes)
            return
        
        results = self.evaluation_results['trained_vs_trained']
        
        # Crear gr√°fico de barras con m√©tricas de coordinaci√≥n
        metrics = ['Eventos de\nCoordinaci√≥n', 'Decisiones\nEstrat√©gicas', 'Balance\nCompetitivo']
        values = [
            results['coordination_rate'],
            results['strategic_rate'],
            1 - results['balance_score']
        ]
        
        bars = ax.bar(metrics, values, alpha=0.7, color=['green', 'blue', 'orange'])
        
        for bar, value in zip(bars, values):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                   f'{value:.2%}', ha='center', va='bottom', fontweight='bold')
        
        ax.set_ylabel('Tasa')
        ax.set_ylim(0, 1)
        ax.grid(True, alpha=0.3)
    
    def _plot_performance_summary(self, ax):
        """Resumen de rendimiento general"""
        ax.set_title('üìà Resumen de Rendimiento', fontweight='bold')
        
        # Crear radar chart o gr√°fico de barras con m√©tricas clave
        performance_metrics = {}
        
        if 'vs_random' in self.evaluation_results:
            vs_random = self.evaluation_results['vs_random']
            performance_metrics['Superioridad vs Aleatorios'] = vs_random['trained_win_rate']
            performance_metrics['Decisiones Estrat√©gicas'] = vs_random['strategic_rate']
        
        if 'trained_vs_trained' in self.evaluation_results:
            vs_trained = self.evaluation_results['trained_vs_trained']
            performance_metrics['Coordinaci√≥n'] = vs_trained['coordination_rate']
            performance_metrics['Balance Competitivo'] = 1 - vs_trained['balance_score']
        
        if performance_metrics:
            metrics = list(performance_metrics.keys())
            values = list(performance_metrics.values())
            
            # Crear gr√°fico de barras horizontal
            bars = ax.barh(metrics, values, alpha=0.7, color=['green', 'blue', 'orange', 'purple'][:len(metrics)])
            
            for bar, value in zip(bars, values):
                ax.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,
                       f'{value:.2%}', ha='left', va='center', fontweight='bold')
            
            ax.set_xlabel('Puntuaci√≥n')
            ax.set_xlim(0, 1)
            ax.grid(True, alpha=0.3)
        else:
            ax.text(0.5, 0.5, 'No hay datos\nsuficientes', ha='center', va='center', transform=ax.transAxes)
    
    def _generate_text_summary(self):
        """Generar resumen textual de la evaluaci√≥n"""
        summary_path = os.path.join(self.results_dir, "evaluation_summary.txt")
        
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("üéØ RESUMEN DE EVALUACI√ìN - AGENTES MARL MUS GAME\n")
            f.write("=" * 55 + "\n\n")
            
            f.write(f"üìÖ Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # Evaluaci√≥n vs agentes aleatorios
            if 'vs_random' in self.evaluation_results:
                results = self.evaluation_results['vs_random']
                f.write("ü§ñ EVALUACI√ìN VS AGENTES ALEATORIOS:\n")
                f.write("-" * 35 + "\n")
                f.write(f"üèÜ Tasa de victoria entrenados: {results['trained_win_rate']:.1%}\n")
                f.write(f"üé≤ Tasa de victoria aleatorios: {results['random_win_rate']:.1%}\n")
                f.write(f"ü§ù Empates: {results['draw_rate']:.1%}\n")
                f.write(f"üìä Puntuaci√≥n promedio entrenados: {results['avg_trained_score']:.1f}\n")
                f.write(f"üìä Puntuaci√≥n promedio aleatorios: {results['avg_random_score']:.1f}\n")
                f.write(f"üéØ Tasa de decisiones estrat√©gicas: {results['strategic_rate']:.1%}\n")
                f.write(f"‚è±Ô∏è Duraci√≥n promedio de juego: {results['avg_game_length']:.1f} acciones\n\n")
                
                # An√°lisis de resultados
                if results['trained_win_rate'] > 0.6:
                    f.write("‚úÖ EXCELENTE: Los agentes superan claramente a los aleatorios\n")
                elif results['trained_win_rate'] > 0.55:
                    f.write("‚úÖ BUENO: Los agentes muestran ventaja sobre los aleatorios\n")
                elif results['trained_win_rate'] > 0.45:
                    f.write("‚ö†Ô∏è REGULAR: Los agentes no muestran ventaja clara\n")
                else:
                    f.write("‚ùå MALO: Los agentes no han aprendido efectivamente\n")
                f.write("\n")
            
            # Evaluaci√≥n entre agentes entrenados
            if 'trained_vs_trained' in self.evaluation_results:
                results = self.evaluation_results['trained_vs_trained']
                f.write("ü§ñ EVALUACI√ìN ENTRE AGENTES ENTRENADOS:\n")
                f.write("-" * 38 + "\n")
                f.write(f"‚öñÔ∏è Tasa de victoria Equipo 0: {results['team_0_win_rate']:.1%}\n")
                f.write(f"‚öñÔ∏è Tasa de victoria Equipo 1: {results['team_1_win_rate']:.1%}\n")
                f.write(f"ü§ù Empates: {results['draw_rate']:.1%}\n")
                f.write(f"ü§ù Tasa de coordinaci√≥n: {results['coordination_rate']:.1%}\n")
                f.write(f"üéØ Tasa de decisiones estrat√©gicas: {results['strategic_rate']:.1%}\n")
                f.write(f"üìä Puntuaci√≥n de balance: {results['balance_score']:.3f}\n")
                f.write(f"‚è±Ô∏è Duraci√≥n promedio de juego: {results['avg_game_length']:.1f} acciones\n\n")
                
                # An√°lisis de balance
                if results['balance_score'] < 0.1:
                    f.write("‚úÖ EXCELENTE: Equipos muy balanceados\n")
                elif results['balance_score'] < 0.2:
                    f.write("‚úÖ BUENO: Equipos razonablemente balanceados\n")
                else:
                    f.write("‚ö†Ô∏è DESBALANCEADO: Un equipo domina al otro\n")
                f.write("\n")
            
            # Conclusiones generales
            f.write("üéØ CONCLUSIONES GENERALES:\n")
            f.write("-" * 25 + "\n")
            
            learning_success = False
            if 'vs_random' in self.evaluation_results:
                if self.evaluation_results['vs_random']['trained_win_rate'] > 0.55:
                    learning_success = True
                    f.write("‚úÖ Los agentes han aprendido estrategias efectivas\n")
                else:
                    f.write("‚ùå Los agentes necesitan m√°s entrenamiento\n")
            
            if 'trained_vs_trained' in self.evaluation_results:
                coord_rate = self.evaluation_results['trained_vs_trained']['coordination_rate']
                if coord_rate > 0.3:
                    f.write("‚úÖ Buena coordinaci√≥n entre compa√±eros de equipo\n")
                elif coord_rate > 0.1:
                    f.write("‚ö†Ô∏è Coordinaci√≥n moderada entre compa√±eros\n")
                else:
                    f.write("‚ùå Poca coordinaci√≥n entre compa√±eros\n")
            
            # Recomendaciones
            f.write("\nüí° RECOMENDACIONES:\n")
            f.write("-" * 18 + "\n")
            
            if not learning_success:
                f.write("‚Ä¢ Aumentar el n√∫mero de episodios de entrenamiento\n")
                f.write("‚Ä¢ Ajustar hiperpar√°metros (learning rate, epsilon decay)\n")
                f.write("‚Ä¢ Revisar sistema de recompensas\n")
            
            if 'trained_vs_trained' in self.evaluation_results:
                if self.evaluation_results['trained_vs_trained']['coordination_rate'] < 0.2:
                    f.write("‚Ä¢ Implementar recompensas espec√≠ficas para coordinaci√≥n\n")
                    f.write("‚Ä¢ Considerar comunicaci√≥n entre agentes del mismo equipo\n")
            
            f.write("\nüéÆ Para entrenar m√°s, ejecutar: python improved_training_system.py\n")
        
        print(f"üìã Resumen de evaluaci√≥n guardado: {summary_path}")
    
    def run_quick_evaluation(self):
        """Ejecutar evaluaci√≥n r√°pida completa"""
        print("üöÄ Iniciando evaluaci√≥n r√°pida de agentes MARL...")
        print("=" * 50)
        
        # Cargar agentes entrenados
        trained_agents = self.load_trained_agents()
        
        if not trained_agents:
            print("‚ùå No se pudieron cargar agentes entrenados")
            print("üí° Ejecuta primero el entrenamiento con: python improved_training_system.py")
            return
        
        print(f"‚úÖ Agentes cargados: {list(trained_agents.keys())}")
        
        # Evaluaci√≥n vs agentes aleatorios
        print("\nüé≤ Evaluando contra agentes aleatorios...")
        vs_random_results = self.evaluate_agents_vs_random(trained_agents, num_games=100)
        
        print(f"üìä Resultados vs Aleatorios:")
        print(f"   üèÜ Tasa de victoria: {vs_random_results['trained_win_rate']:.1%}")
        print(f"   üéØ Decisiones estrat√©gicas: {vs_random_results['strategic_rate']:.1%}")
        print(f"   üìà Puntuaci√≥n promedio: {vs_random_results['avg_trained_score']:.1f}")
        
        # Evaluaci√≥n entre agentes entrenados
        print("\nü§ñ Evaluando agentes entrenados entre s√≠...")
        vs_trained_results = self.evaluate_trained_vs_trained(trained_agents, num_games=50)
        
        print(f"üìä Resultados Entrenados vs Entrenados:")
        print(f"   ‚öñÔ∏è Balance competitivo: {1-vs_trained_results['balance_score']:.1%}")
        print(f"   ü§ù Coordinaci√≥n de equipo: {vs_trained_results['coordination_rate']:.1%}")
        print(f"   üéØ Decisiones estrat√©gicas: {vs_trained_results['strategic_rate']:.1%}")
        
        # Generar informe visual
        print("\nüìä Generando informe visual...")
        self.generate_evaluation_report()
        
        # Evaluaci√≥n final
        print("\nüéØ EVALUACI√ìN FINAL:")
        if vs_random_results['trained_win_rate'] > 0.6:
            print("‚úÖ EXCELENTE: Los agentes han aprendido efectivamente")
        elif vs_random_results['trained_win_rate'] > 0.55:
            print("‚úÖ BUENO: Los agentes muestran aprendizaje")
        elif vs_random_results['trained_win_rate'] > 0.45:
            print("‚ö†Ô∏è REGULAR: Aprendizaje limitado")
        else:
            print("‚ùå MALO: Los agentes necesitan m√°s entrenamiento")
        
        print(f"\nüìÅ Resultados guardados en: {self.results_dir}/")
        
        return self.evaluation_results


if __name__ == "__main__":
    evaluator = QuickEvaluationTool()
    evaluator.run_quick_evaluation()
